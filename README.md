# Утилита загрузки веб-страниц вместе со всем вложенным контентом.

## Возможности

- Умеет скачивать HTML-страницы, сохранять их локально, а также рекурсивно скачивать ресурсы: CSS, JS, изображения и т.д., а так же страницы, на которые есть ссылки (в рамках того же домена).

- На выходе получается локальный каталог, содержащий копию сайта (или его части), страницу можно открыть офлайн.

- Обрабатываются различные нюансы: относительные и абсолютные ссылки, дублирование (один и тот же ресурс не скачивается несколько раз), корректно формируются локальные пути для сохранения, избегаются зацикливания по ссылкам.

- Поддерживает параллельное скачивание.

## Флаги

- `-u` - URL страницы для скачивания. Обязательный флаг;
- `-d` - Глубина рекурсии. Опциональный флаг, по умолчанию - 1;
- `-n` - Максимальное количество одновременных загрузок. Опциональный флаг, по умолчанию - 5;
- `-t` - Таймаут в секундах для HTTP-клиента. Опциональный флаг, по умолчанию - 30;

## Пример использования

Для удобства можем выполнить(в примере запускаю так):
```
go build -o mywget main.go
```

Используем:
```
./mywget -u https://go.dev/doc/
```
Получаем директорию `go.dev`, в которой:
<img width="612" height="491" alt="image" src="https://github.com/user-attachments/assets/39e7d233-c67d-4751-9ef9-0bb65cc7c262" />

Множество директорий, перейдем в одну из них:
<img width="610" height="193" alt="image" src="https://github.com/user-attachments/assets/a52b3c84-8f78-4761-9539-e261e4ac06c5" />

Видим саму копию страницы и необходимые картинки.

CSS и JS файлы также скачиваются, страницы их используют.
<img width="599" height="335" alt="image" src="https://github.com/user-attachments/assets/378e8986-56b2-45c1-b49b-656ae3361add" />
<img width="604" height="39" alt="image" src="https://github.com/user-attachments/assets/b0d34948-50c3-41d6-b42d-6a3e114fa072" />

В страницах ссылки замененены на относительные локальные, поэтому по ним можно переходить.
